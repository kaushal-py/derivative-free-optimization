<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Derivative-free Optimization</title>
</head>
<script src="template.v1.js"></script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<dt-article>
  <h1>Derivative-free Optimization</h1>
  <h2>Tutorial for Genetic algorithms and Simulated Annealing</h2>
  <dt-byline></dt-byline>

  <h2>Genetic Algorithms</h2>
  
  <h3> Optimization - a very , very brief primer! </h3>
  
 <p>  So really, what is optimization? Since this post talks about <i> Derivative - Free Optimization </i> so to speak, it is assumed that the reader is familiar with the basics of optimization. If you are not, no need to fret this section provides a very, very brief intro, frankly, 
  there is no need as such to have a thorough foundation on the underlying mechanism of optimization to understand the following discussion. But like many frameworks, the entire reason why Derivative free methods exist, such as the ones discussed here, Genetic Algorithms and Simulated Annealing
  is based on some of the issues encountered in the old-school methods. So it would be a great idea to have some superficial understanding of what's going on, just so that we can appreciate the innovative ideas that have solved them </p>
  
  </p>
  
  <p> Now, to business. Optimization at a very , very basic level is to solve a certain problem <i> optimally </i>. For example, if you have your final exams for the semester, How would you try to maximize your marks? Or how would a logistics company
      decide to deliver parcels in a particular city while ensuring that it takes the least amount of time and fuel , cause well, time is money and fuel ain't cheap! There are many, many such problems at the heart of the modern world
      Many modern techniques such as the ever-so popular Machine Learning that has become so omnipresent today from chatbots and virtual assistants to giving Recommendations on Netflix on what to watch next are also at their very core - optimization paradigms
      Since optimization is so useful, it stands to reason that bottlenecks affecting optimization procedures should be quite important as well and indeed they are, as a matter of fact billions of dollars are spent in R&D just to get rid of these bottlenecks.
      One such bottleneck that has stymied the devolopment of better algorithms has been the over-reliance on <i> "derivatives" </i>. An enormous majority of today's optimization algorithms including those all-powerful Neural - Networks rely a great deal on <i> Derivative </i> based
      methods. To understand what a derivative based method does consider the following ; given some function that you want to optimize (<i> henceforth : the objective function </i>) say, </p> <p> $$ f(x) $$ </p><p>
      the method would use its derivative to get some information to reach the optimal solution. High school math is enough to let us know that</p> <p> $$ f'(x) = 0 $$ </p> <p> at an extremum and a basic knowledge of multivariable calculus extends this to </p> <p>
        $$ \nabla f(\bf{x}) = \bf{0} $$ </p> <p>
          
      If you want more information to <i> converge </i> faster to a solution, then you can use higher derivatives or their approximations. Modern methods like Automatic Differentiation makes computing derivatives easy and since the algorithms using these
      are straightforward to implement and have robust guarantees of <i> "converging" </i> , they are used quite a great deal. Even so, they do suffer from certain issues that necessitates the ideas discussed in our post. We will be looking into those in the next section, and that's all for
      the basics of Optimization. For a more detailed introduction on optimization and derivative based methods, there are some great videos and courses out there on You tube , not to mention the bible - Numerical Optimization by Nocedal and Wright which is a great read.
    
</p>

<h3> Why do we need Derivative free methods? </h3>
<p> <img src="images/ga_motivation.jpg" alt="GA">
  
  <br>
    
  </p>
  
  <p> The above figure shows a realistic situation that happens when we use Derivative based methods. If you could recall from above, the idea behind derivative based methods is to use characteristics
      of the derivatives of the objective function to get closer to an optimal value. This works quite well for your usual sort of functions that are well behaved such as convex functions ("Bowl shaped functions" : Your task 
      is to reach the bottom of the bowl, the derivative provides the direction of steepest descent and then whoosh! Off you go! Just to see why this is so fast, take a bowl and use your fingers to replicate the process, you would reach the bottom in no time!) 
      but goes wonky for other functions that are not so well behaved. This is because "derivatives" are unable to distinguish between <i> locally </i> and <i> globally </i> optimal solutions. 
           
  </p>

  <p> To understand what this means, look at the figure above, Suppose you are a mountaineer out to conquer the highest mountain on a range, you have got a bunch of peaks there on your landscape. Look at the point labelled <i> Start Here Randomly </i>, what derivative based methods would do is to start climbing upward,
      they would ensure that you get to the top of the nearest hill, but then you're stuck! You can't take a single step ahead because the derivatve which is sort of a slope monitor tells you that the terrain doesn't go upward from here !
      So according to your monitor you have reached the peak, but you being a well-prepared person have already done your survey and are aware beforehand that there is a much higher peak ahead! So now you are stuck and angry, but fret not
     there were many others in the same fix as you were while going through different applications in engineering and science, applications that are quite necessary to the functioning of various aspects of society that we take for granted today.
     So the answer to this problem is to include an entirely new monitor to understand the terrain - i.e, to consciously chose to avoid using the <i> derivative </i> which is why these methods are called derivative-free methods.
     There are other reasons to adopt derivative-free based methods but they are mostly <i> computational </i> in nature i.e relate to the problems encountered in getting the derivative in the first place. The above discussed "landscape problem "
     shows on the other hand a much more intuitive need for going "beyond" the derivative in the first place. Many of the methods we will discuss below are often grouped together in the banner of <i> meta-heuristics </i> (Meta : Latin for beyond,
     heuristics - methodology of solving a problem, in this case referring to the derivative based methods.) 
    
</p>

<p> Now, that we have a certain need for Derivative free methods. Let's get cracking ! </p>


<h3> Genetic Algorithms </h3>

<p> Before we begin, We would like to state that the <i> phenomenon </i> of optimization is far more general than we thought. While the way in which humans perform decision making can be modelled as an optimization scenario, the broader behaviour of nature may be modelled 
    through an optimization perspective as well. To understand what we are trying to say, look at the figure below
</p> 
<p>   <img src="images/pso.JPG" alt="PSO"> <br> </p>
  
  <p> The image shows a flock of birds trying to find some fruit to eat. From observation, people observed that any movement of the flock followed three simple rules : 
</p> 

  <ul> <li> Move in the same direction as your neighbours </li>
       <li> Stay close to your neighbours </li>
       <li> Avoid collisions with your neighbours   
  </ul>
<p>
  Surprisingly, these simple rules ensure that the flock finds food in the shortest period of time! What we have therfeore is an optimization paradigm where the problem is to find sufficient food for the entire flock in the shortest possible of time! Many such examples
  are abound in nature and since the behaviour of nature is often based on certain simple rules (Wouldn't be so widespread otherwise! If birds had to sit down and run robust calculations to calculate how to get food the flock would have died out long ago), they tend to be 
 <i> derivative -free </i> and quite general. For example , the following amazing article <a href = "https://www.quantamagazine.org/math-of-the-penguins-20200817/" >Math of the Penguins </a> shows how a flock of Emperor penguins huddle in the most efficient structure possible in order to keep warm
 in the frigid winter, all based on the simple rule that every penguin wants to stay warm and would thus migrate to a warmer location to get the warmth it needs. The innate simplicity, generality and the <i> understanding </i> that such methods can converge to the optimal solution inspired scientists 
 and engineers to design many nature inspired algorithms for optimization. (Note : we mentioned that nature inspired paradigms "could" converge to the solution, till date there is no rigid theory on how and why these methods can converge thus making it more of an empirical framework which is why
 it is often not preferred despite its performance. Moreover, the selection of parameters is always a challenge. For our discussion, we would sweep such issues under the rug for the most part while keeping in mind that they exist, since these methods are quite effective practically which makes it worth
 to have a proper discussion!)

</p>

<p> The paradigm that we are going to discuss in this section - namely, Genetic Algorithms are a specific kind of nature inspired optimization algorithms which are inspired from genetics and Darwin's theory of evolution </p>

<p> <h4> Another brief summary - This time a primer on biology! </h4> </p>

<p> The theory of genetics and Darwin's theory on evolution are so widespread that an introduction sounds redudant. Even so, we provide a brief refresher in this section with the definition of some terms that will be encountered in our discussion but are not so well known.
</p>

<p>
  
The genetic basis of inheritance suggests that the way individuals of a species pass on their traits to their offspring is through information encoded in their genes, which are basically sections of chromosones which are themselves a bunch of DNA molecules, found in the cells
of the individual organisms. How genes interact in order to come up with the various traits such as height or colour of the eyes is beyond the scope of any introductory material but we may assume for the sake of simplcity that genes have certain possible values termed as <i> alleles </i>.
Different alleles in a chromosome together determine how a certain trait is expressed. We define any observable characteristic such as height or the colour of eyes to be a <i> phenotype </i>. All the alleles displayed in the various chromosomes that determine a phenotype are the 
<i> Genotype </i>.
  
</p>

<p>Now, we come to the discussion on evolution. If we assume that the genotype of an organism remains constant throughout its existence then we have no diveristy - which is not ideal given that the environment is always changing and thus, forcing the organism and the species to respond in different ways.
Thus, the changing environment throughout the course of our history has forced organisms to respond in different sorts of ways in order to avoid becoming extinct , i.e by showcasing different phenotypes which implies that we need different genotypes. Thus, we come to the concept of darwinian evolution, i,e "Survial of the fittest", 
Therefore "evolution" which for our case we assume has the meaning that the organism <i> shows the optimal fitness in a given environment </i> has to proceed by a modification of the genotype guided by some <i> fitness measure</i> so that the organism can survive better. 
Since, the genotype has to be modified in some manner, how can organisms go about doing this? The first and the easiest way, is to change the genotype on their own, since there is no explicit way for organisms to explicitly tell their genotype to change in a certain way, any such change has to be <i> random </i>  in nature(Btw , 
if you are interested the recent Nobel Prize in medicine was given for the CRISPR method which is an innovative method that allows for explicit editing of the genotype). Any such random change or <i> mutation </i> (You would have encountered this term in the X-Men movies for example, with exactly the same meaning
!) allows the organism to showcase different phenotypes, allowing it to adapt better to the changing environment during its lifetime. But clearly , mutation changes do not seem to allow for increase in fitness of the species as whole! Thw way nature solves this problem is by ensuring that during sexual reproduction when new
offpsring are generated, there is a significant possibility for traits from either of the two parents to be passed on to the offspring, again as there is no regulatory procedure which trait gets passed on to which offspring is entirely stochastic however over a long time span the proportion of the benificial phenotype will be magnified
since the organisms without the said benificial trait would have died out! This explains how evolution and extinction of species occurs in nature. This transfer of trait from the genes of the two parents is termed as "crossing-over" (a detailed description of the nomenclature would require a digression on meiosis which is the type of cell
division occuring in the reproductive or "germinal" cells such as the sperm or the egg.) Intuitively, one can think of the crossing over process in the following manner : keep two chromosomes next to each other take some alleles from one and interchange it with the other to get a certain resultant chromosome. This is also made clear in the figure below:

</p>
<p> <img src = "images/crossing_over.png" , alt = "crossing over"> <br> </p>
  
<p> The above image shows how crossing over occurs in genes during cell division. That's all the information we would need from biology. Now with the intution in place let's get to how we would be actually using "Genetic Algorithms" 
  
</p>

<p> <h4> The formulation of Genetic Algorithms </h4> </p>

<p>  Taking inpiration from our earlier discussion, our objective function should represent the environment in some sense and suppose we have a bunch of "solutions" or a <i> population </i> of initial candidates through evolution we should get a species that shows optimal <i> fitness </i> for our situation. 
     To ensure that our formalism is consistent with the biological one, define a chromosome to be an array of possible values that represent our solution candidate. A gene is a certain index in our array and the value at that index is the allele. The set of chromosomes we have at one point can be said to
     represent our genotype. The alleles themselves can be binary , integer or real valued depending on our choice of representation, our phenotype in this case represents the actual objective function and the values lying in its domain. We modify our genotype based on a fitness function that is designed to
     ensure that our final genotype represents a phenotype that solves our objective function optimally. Lastly, noting that the genotype and phenotype may have entirely different sets of values forces us to contruct a mapping from the possible values of the phenotype to the genotype and back so that we can get 
     the genotype behind a phenotype and a phenotype for a certain genotype. The following figures showcase our formalism.
  
  
</p>

<p>

<img src = "images/ga_basic.JPG" , alt = "Representation"> <br>

<img src = "images/ga_2.JPG" , alt = "Genotype to phenotype"> <br>
</p>


<p>
  Naturally, the full power of our evolutionary mechanism wouldn't work without including the phenomena of <i> mutation </i> and <i> crossing-over </i>. We can do this in the following way, with a certain probability <i> M <sub> p </sub> </i>
  we perform an operation on a chromsome such as swapping the values of two indices or randomly changing the value at a particular gene etc, such operations clearly are an analogue of mutation of natural genes. A crossing over now, is any operation that combines two chromosomes in a certain way to get a new resultant chromosome. Now, suppose we have an initial population, say <i> F <sub> 0 </sub> </i> after doing an initial fitness calculation we mutate individuals and then perform crossing-over to get a new population, <i> F <sub> 1 </sub> </i>.
  Now, it may be the case that <i>F <sub> 1 </sub> </i> has much more individuals than <i>F <sub> 0 </sub> </i> in order to avoid increasing storage complexity with excessive redundant candidates, we take a leaf out of Darwin' book and only consider the <i> Fittest</i>, this so called <i> elitism </i>, is one of many selection procedures which are used to ensure that succesive generations have controllable population.
  With the basic formalism sorted and remembering our intuition from Darwinian evolution, we can construct an algorithmic procedure for performing a Genetic Algorithm as follows : 
</p>   
<p> <img src = "images/ga_exec.JPG" , alt = "Execution of a Genetic Algorithm"> <br> </p>
  
  

<p> <h3> Some tricks of the trade on How to implement a Genetic Algorithm </h3> </p>

<h4> <p>  How to initialize the population? </p> </h4> 

<p> We can choose to either populate our initial choices randomly or by employing a certain assumption. While the fomer method may converge much slowly after several rounds of experimental research it has been proven to converge to a more optimal solution, intuitively this is because the latter method has lesser "diversity" and thus can only show a certain number of possible phenotypes.
The trick that is used commonly these days is to combine both methods, add several randomly initialized chromosomes while adding some "good" ones as well, this can cause the convergence to be fast while ensuring that the advnatge of randomization is not lost as well.
</p> 


<h4> <p>  How to define a fitness measure?  </p> </h4>
<p> This is a very tricky issue since there isn't any standard way to define a fitness measure with the measure being highly specific to the task in hand. But here are some rough guidelines that work often in practice :
</p>
<ul>
<li> A good fitness measure should be easy to evaluate i.e easy to compute, saving on the algorithm running time </li>

<li> A good fitness measure should be as well-behaved as possible i.e properties such as continuity and differentiability are a big bonus and if the alleles are integer based or binary then a function like the digital sum i.e the sum of all non-zero values is a good choice if the problem allows

<li> Most importantly, a good fitness measure should <i>mimic</i> the actual objective function i.e optimizing the fitness function in the genotype space should be equivalent to optimizing the objective in the phenotype space.
 
</ul>
<p> The last requirement is the most tricky, but also the most necessary one, otherwise our Genetic algorithm would simply not work!. In fact, the priority order to be followed is 3>2>1 when designing a fitness measure but it's crucial to ensure that we don't abandon lower priority requirements entirely.
</p> 

<h4> <p> How to choose Parents? </p> </h4>

<p> To understand why we need this discussion in the first place, let's go back to the problem that motivated survival selection. Once we perform a crossing-over on the entire population we end up getting too many possibilities for the next generation! Apart from survival selection which is needed to reduce the storage complexity,
We also need to reduce the time complexity of the crossing-over step somehow in order to get a practically fast algorithm. The easiest way to do this is by ensuring that we can choose the right sort of "parents". That is by performing the crossing over only on a certain subset of the population we can ensure that diversity is propagated optimally whilst
not overburdening our resources this is inspired from nature too, specifically the phenomenon of <i>Sexual Selection</i> which was also discussed by Darwin in his seminal work, <i>On the orgin of Species </i>. The colourful plummage of peacocks for example is an evolutionary adaptation that allows for a peahen to select an optimal mate.
We would not go in to sexual selection in detail here, but there are a great many works out there that explain and illustrate this idea beautifully. To get back to the issue we have at hand, some strategies that are used in practice for parent selection are as follows :
</p>

<ul>
<li>  <p> Fitness Proportionate Selection is one of the most popular ways of parent selection. In this every individual can become a parent with a probability which is proportional to its fitness. Therefore, fitter individuals have a higher chance of mating and propagating their features to the next generation. Therefore, such a selection strategy applies a selection pressure to the more fit individuals in the population, evolving better individuals over time.

Consider a circular wheel. The wheel is divided into n pies, where n is the number of individuals in the population. Each individual gets a portion of the circle which is proportional to its fitness value.

  Two implementations of fitness proportionate selection are used in standard practice  </p>
<ol>

  <li> <p> In a roulette wheel selection, the circular wheel is divided as described before. A fixed point is chosen on the wheel circumference as shown and the wheel is rotated. The region of the wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.
    
    It is clear that a fitter individual has a greater pie on the wheel and therefore a greater chance of landing in front of the fixed point when the wheel is rotated. Therefore, the probability of choosing an individual depends directly on its fitness.

    Implementation wise, we use the following steps − </p>
    
    <ul> 
       <li> Calculate S = the sum of a fitnesses </li>
      <li>  Generate a random number between 0 and S. </li>
      <li> <p> Starting from the top of the population, keep adding the finesses to the partial sum P, till P exceeds S </p> </li>
      <li>  The individual for which P exceeds S is the chosen individual</li>
    </ul>
    
    <p> The following image shows roulette wheel population. </p> 
    
    <p> <img src = "images/pa_1.JPG"> <br> </p>
 </li>
  
  <li> <p> Stochastic Universal Sampling is quite similar to Roulette wheel selection, however instead of having just one fixed point, we have multiple fixed points as shown in the following image. Therefore, all the parents are chosen in just one spin of the wheel. Also, such a setup encourages the highly fit individuals to be chosen at least once.
  
    An image showcasing this procedure is as follows : </p>
    
    <p> <img src = "images/pa_2.JPG"> <br> </p>
 </li> </ol>
  
 <p> It is to be noted that fitness proportionate selection methods don’t work for cases where the fitness can take a negative value. </p>
  
</li>

<li>
  <p> Our next strategy is the so called Tournament Selection. In K-Way tournament selection, we select K individuals from the population at random and select the best out of these to become a parent. The same process is repeated for selecting the next parent. Tournament Selection is also extremely popular in literature as it can even work with negative fitness values.
    The following image showcases this strategy.
  </p>
  
  <img src = "images/pa_3.JPG"> <br>
  </li>
  
  <li> <p> We will now look at Rank Selection. Rank Selection also works with negative fitness values and is mostly used when the individuals in the population have very close fitness values (this happens usually at the end of the run). This leads to each individual having an almost equal share of the pie (like in case of fitness proportionate selection) as shown in the following image and hence each individual no matter how fit relative to each other has an approximately same probability of getting selected as a parent. This in turn leads to a loss in the selection pressure towards fitter individuals, making the GA to make poor parent selections in such situations. </p>
    
    <img src = "images/pa_4.JPG"> <br>
    
    <p>  In this, we remove the concept of a fitness value while selecting a parent. However, every individual in the population is ranked according to their fitness. The selection of the parents depends on the rank of each individual and not the fitness. The higher ranked individuals are preferred more than the lower ranked ones.</p>
   
    <img src = "images/pa_5.JPG"> <br>
  </li>
</ul>

<h4><p> How to perform mutations , crossing-over and Survival Selection? </p> </h4>

<p> We would not go into detail on the procedure of performing mutations and crossing-over, simply because there are too many possibilites. The important thing is to note that for mutation with a certain probability we randomly change individual chromosomes and for 
crossing-over we are expected to use two chromosomes in order to get a third one. There are many means to perform survival selection apart from the <i> elitism </i> discussed earlier there is also the <i> aging </i> procedure in which the indvidual from a certain generation 
if it remains as such for a predetermined N  number of succesive generations then it will be automatically removed at the N+1 <sup> th </sup> succesive generation. This procedure clearly mimics the process of aging and death found in nature.
There are even more means by which we can ensure that survival selection but the key point is to note that we trim the population after every generation. </p>

<p> <h4>How to know that convergence has been achieved? </h4> </p>

<p> It has been observed that initially, the Genetic Algorithm progresses very fast with better solutions coming in every few iterations, but this tends to saturate in the later stages where the improvements are very small. We usually want a termination condition such that our solution is close to the optimal, at the end of the run.

Usually, we keep one of the following termination conditions : 

<ul>
  <li> When there has been no improvement in the population for X iterations.</li>
  <li> When we reach a certain Y number of generations. </li>
  <li> When the objective function value has reached a certain pre-defined value. </li>
</ul>
</p>

<p> It is important to note that all the tricks discussed above are highly generic and that is upto the designer of the algorithm to ensure how they implement the algorithm for the problem in hand. Even so these rough guidelines, showcase a certain direction that can be kept in mind
  for a designer to get a more efficient algorithm. </p>


  <h3>Demo Problem</h3>
  
  <p><p> In this section, we solve a problem step by step using python code in order to illustrate the concepts that have been discussed above. Let us look at the graph of a simple 1-D function. $$f(x) = sin(x)+sin(\dfrac{10x}{3})$$ </p>

  <p>
    <img src="images/ga_example_function.png" alt="ga demo">
  </p>

   <p>The function has 4 local minimas in the range (0, 8). Gradient-based methods are very efficient methods to find local minimas. But, such algorithms would fail when presented with such a function and asked to find the global minima. It will heavily depend on the starting condition, if the solution found is global or not. For this example, if the initial point was in the range (4.5 , 6) it would find the global minima. </p>
  
  <p> Let us now try and use the newly learnt Genetic algorithm, and test out how it works on this problem. But first, we need to implement the algorithm. So let's get ready to get our hands dirty.</p>

  <h4>Genotype Representation</h4>
  <p>The first step in the genetic algorithm is to formulate the solutions to the problem (chromosones) as a binary string. This is sometimes the most crucial step in formulating a GA, and convergence may heavily depend on the design choices considered here.</p>
  
  <p>For us, the given problem is a 1-D function that takes a real number as an input and returns a real value. We know that the domain of the function is between (0,8). We can this observation to our advantage. The idea is quite simple, and here it is. Let's say we want to represent 2.048 in a binary representation, we first multiply it by 1000 to get 2.049*1000 = 2049, and then find the binary representation of the number which is 100000000001. 
    To represent all numbers between 0 and 8 with upto 3 decimal places, we could use a 13-bit binary representation (2^13 = 8192 numbers) and use it to represent all numbers with 3 decimal places between (0, 8.192).</p>
  
  <p>Thus, we get the real valued representation for any 13-bit chromosome. In python, we will define a function for the conversion:</p>

  <dt-code block language="python">
def get_chromosome_representation(x):
  return int(x, 2)/1000
  </dt-code>

  <p>Let us look some random binary strings and their corresponding values - </p>

  <dt-code block language="python">
0110111111100 => 3.58
1000001011001 => 4.185
1110101011011 => 7.515
0010111110101 => 1.525
1110100110101 => 7.477
0000000000000 => 0.0
1000000000000 => 4.096
  </dt-code>
  
 <h4>Population Initialization</h4>
 <p>We initialise the population of chromosomes randomly. We sample random 13-bit strings. We use the numpy libray for this purpose. The size of the population is an important choice to make. Let's generate a population of size 10.</p>
 
<dt-code block language="python">
def init_population(population_size, chromosome_size):
    
  np.random.seed(0)
  population = np.random.randint(2, size=(population_size, chromosome_size))
  population = [''.join([str(i) for i in chrom]) for chrom in population]
  return population

population = init_population(10, 13)
population

Output>
['0110111111100',
 '1000001011001',
 '1110101011011',
 '0010111110101',
 '1110100110101',
 '0000011000110',
 '1001011111101',
 '1001001101001',
 '0001101000001',
 '0101111101111']
</dt-code>
  
<h4>Fitness function</h4>
<p>We have a minimization objective for our function. Generally a fitness function is a metric for how fit the candidate is, thus, the larger the fitness the better. Thus, we use -f(x) as the fitness function. This ensures that our minima will have the maximum fitness. </p>

<dt-code block language="python">
def f(x):
  return np.sin(x) + np.sin(10*x/3)

def fitness_function(population):
  '''
  Calcualtes the fitness of the population
  Returns the fitness score for the entire population
  '''
  vals = [int(n, 2)/1000 for n in population]
  fitness = [-f(x) for x in vals]
  population = np.expand_dims(population, 1)
  fitness = np.expand_dims(fitness, 1)
  data = np.concatenate([population, fitness], axis=1)
  data = [[x, round(float(y), 4)] for [x,y] in data]
  return data

data = fitness_function(population)
print("Chromosomes and their fitness values")
data
  
Output>
Chromosomes and their fitness values
[['0110111111100', 1.0161],
 ['1000001011001', -0.1184],
 ['1110101011011', -0.8604],
 ['0010111110101', -0.067],
 ['1110100110101', -0.7219],
 ['0000011000110', -0.8098],
 ['1001011111101', 1.4643],
 ['1001001101001', 1.002],
 ['0001101000001', -1.0968],
 ['0101111101111', 0.6014]]
  </dt-code>

  <h4>Parent Selection</h4>
  <p>We use k-way tournamet selection for parent selection.</p>

  <dt-code block language="python">
def tournament_selection(data, k=5, num_parents=10):
    # Shuffle the data
    parents = []
    # Select parents
    for i in range(num_parents):
      # Select k samples and take the one having max fitness
      np.random.shuffle(data)
      parent = sorted(data[:k],key=lambda l:l[1], reverse=True)[0][0]
      parents.append(parent)
    return parents

parents = tournament_selection(data, 5, 5)
parents
      
Output>
['1001001101001',
 '1001011111101',
 '0010111110101',
 '1001011111101',
 '1001001101001']
</dt-code>

<h4>Crossover</h4>
<p>We use uniform crossover to produce new offsprings.</p>
<dt-code block language="python">
def uniform_crossover(parents, num_children=5):
  '''
  Performs uniform crossover given a set of parents.
  Two parents are selected randomly from the pool of parents for crossover.
  '''
  children = []
  # for i in range(self.population_size):
  for i in range(num_children):
    p1 = np.random.choice(parents, replace=False)
    p2 = np.random.choice(parents)
    p = [p1, p2]
    child = ''
    for i in range(len(p1)):
      toss = np.random.randint(2)
      child += p[toss][i]
    print("{} x {} => {}".format(p1, p2, child))
    children.append(child)
  return children

children = uniform_crossover(parents, num_children=5)

Output>
1001011111101 x 1001011111101 => 1001011111101
1001001101001 x 1001001101001 => 1001001101001
1001001101001 x 1001001101001 => 1001001101001
1001001101001 x 1001011111101 => 1001011111101
0010111110101 x 1001001101001 => 0000001100101
  </dt-code>

<h4>Mutation</h4>
<dt-code block language="python">
def mutation(population, prob=0.01):
    mutated = []
    for chrom in population:
      new_chrom = ''
      for i in range(len(chrom)):
        toss = np.random.random()
        gene = chrom[i]
        if toss < prob:
          gene = str(1-int(gene))
        new_chrom += gene
      mutated.append(new_chrom)
    return mutated

mutated = mutation(children, prob=0.1)
result = list(zip(mutated, children))
for i in range(len(result)):
  print("{} +=> {}".format(result[i][0], result[i][1]))
  
Output>
1000011101101 +=> 1001011111101
0001001001011 +=> 1001001101001
0001001101001 +=> 1001001101001
1001111111101 +=> 1001011111101
0000001010101 +=> 0000001100101
    </dt-code>

<h4>Survivor Selection</h4>
<p>
  We remove the most unfit parents and replace them by the newly generated offpsrings and set them as the new population for the next generation.
</p>
<dt-code block language="python">
def survivor_selection(data, children):
  data = sorted(data,key=lambda l:l[1], reverse=False)
  count = len(children)
  population = np.array(data)[:,0]
  population[:count] = children
  return list(population)

survivor_selection(data, mutated)

Output>
['1000011101101',
 '0001001001011',
 '0001001101001',
 '1001111111101',
 '0000001010101',
 '0010111110101',
 '0101111101111',
 '1001001101001',
 '0110111111100',
 '1001011111101']
  </dt-code>

  <h3>A fully working example</h3>
  <p>Now that we have looked at and understood each and every component of the genetic algorithm individually, let's look at a fully working example. To make the code cleaner, we will first define a python class, and then put the functions as the methods of the class. 
  </p>

  <dt-code block language="python">
class GeneticAlgorithmSolver:
  '''
  Solves genetic algorithm for the problem sin(x) + sin(10x/3)
  '''

  def __init__(self, population_size, chromosome_size):
    '''
    Initialise the population randomly
    '''
    np.random.seed(0)
    self.population = np.random.randint(2, size=(population_size, chromosome_size))
    self.population = [''.join([str(i) for i in chrom]) for chrom in self.population]
    self.population_size = population_size
  
  def f(self, x):
    '''
    Objective function
    '''
    return np.sin(x) + np.sin(10*x/3)
  
  def fitness_function(self, population):
    '''
    Calcualtes the fitness of the population
    '''
    vals = [int(n, 2)/1000 for n in population]
    fitness = [-self.f(x) for x in vals]
    population = np.expand_dims(population, 1)
    fitness = np.expand_dims(fitness, 1)
    data = np.concatenate([population, fitness], axis=1)
    data = [[x, float(y)] for [x,y] in data]
    avg_fitness = sum(fitness) / len(fitness)
    return data, avg_fitness
  
  def tournament_selection(self, data, k=5, num_parents=5):
    '''
    Tournament selection strategy for selecting parents
    '''
    # Shuffle the data
    parents = []
    # Select parents
    for i in range(num_parents):
      # Select k samples and take the one having max fitness
      np.random.shuffle(data)
      parent = sorted(data[:k],key=lambda l:l[1], reverse=True)[0][0]
      parents.append(parent)
    return parents
  
  def uniform_crossover(self, parents, num_children=5):
    '''
    Generate new offsprings using uniform crossover
    '''
    children = []
    # for i in range(self.population_size):
    for i in range(num_children):
      p1 = np.random.choice(parents, replace=False)
      p2 = np.random.choice(parents)
      p = [p1, p2]
      child = ''
      for i in range(len(p1)):
        toss = np.random.randint(2)
        child += p[toss][i]
      # print("{} x {} => {}".format(p1, p2, child))
      children.append(child)
    return children
  
  def mutation(self, population, prob=0.01):
    '''
    Perform bit-flip mutation
    '''
    mutated = []
    for chrom in population:
      new_chrom = ''
      for i in range(len(chrom)):
        toss = np.random.random()
        gene = chrom[i]
        if toss < prob:
          gene = str(1-int(gene))
        new_chrom += gene
      mutated.append(new_chrom)
    return mutated
    
  def survivor_selection(self, data, children):
    '''
    Replace the weakest parents by the newly generated offsprings
    '''
    data = sorted(data,key=lambda l:l[1], reverse=False)
    count = len(children)
    population = np.array(data)[:,0]
    population[:count] = children
    return population
  
  def plot_objective(self):
    '''
    Plot the objective function
    '''
    x = np.linspace(0, 9, 100)
    plt.plot(x, self.f(x), linewidth=2)

  def visualize_population(self, population, generation):
    '''
    Visualise the population over generations to see how they improve
    '''
    vals = [int(n, 2)/1000 for n in population]
    f_val = [self.f(x) for x in vals]
    self.plot_objective()
    plt.scatter(vals, f_val, color='#ec4646', zorder=10, s=40, label='candidates')
    plt.title("Generation = {}".format(generation))
    plt.xlabel("x")
    plt.ylabel("f(x)")
    plt.legend()
    plt.savefig("generation_{}.png".format(generation))
    plt.show()

  def get_solution(self, data, generation):
    '''
    Get the solution for the best chromosome
    '''
    best_chromosome = sorted(data,key=lambda l:l[1], reverse=True)[0][0]
    value = int(best_chromosome, 2)/1000
    print("The soultion is {} with objective value {}".format(value, self.f(value)))
    print("Number of generations taken: {}".format(generation))
    return value, self.f(value)

      
  def optimize(self, max_iter=100, early_stopping=20, mutation_prob=0.01, 
                num_children=15, tournament_k=5, num_parents=10, plot=False):
    '''
    The main optimization loop
    max_iter: Maximum number of generations to run
    early_stopping: Stop if no improvement in chromosome for several iterations
    mutation _prob: Probability of mutation for each gene
    num_children: Number of children to generate each generation
    num_parents: Number of parents to generate each generation
    tournament_k: k for k-way tournament selection
    '''
    best_fitness = -100
    no_update_count = 0
    for generation in range(max_iter):
      data, fitness = self.fitness_function(self.population)
      parents = self.tournament_selection(data, k=tournament_k, num_parents=num_parents)
      children = self.uniform_crossover(parents, num_children=num_children)
      mutated_children = self.mutation(children, prob=mutation_prob)
      new_population = self.survivor_selection(data, mutated_children)

      if plot:
        self.visualize_population(self.population, generation)

      self.population = new_population
      # print("Generation {}: Fitness value: {}".format(generation, fitness))
      if fitness > best_fitness:
        best_fitness = fitness
      else:
        no_update_count += 1
      if no_update_count >= early_stopping:
        break
    
    return self.get_solution(data, generation)
      </dt-code>
  
  <p>Yay! All the hardwork is done. Now that we have our working example, let's run it!!</p>

  <dt-code block language="python">
g = GeneticAlgorithmSolver(population_size=20, chromosome_size=13)
g.optimize(early_stopping=3)
      
Output>
The soultion is 5.146 with objective value -1.8995989311583412
Number of generations taken: 10
</dt-code>

<p>Great! The solution matches with the actual global minima. Let us visualize how the genetic algorithm performs as the generations progress.</p>

<p>
  <img src="images/demo.gif" alt="ga demo">
</p>

<p>It is clearly seen that the candidates keep getting better as the generations progress and finally all of them converge to the global minima, which is when the algorithm stops.</p>

<h4>Exercise</h4>
<p>Try changing the hyper-parameters, i.e, the population size, number of parents, number of children, mutation probability and see the effect on covergence.</p>

<h2>Simulated Annealing</h2>
  <p>Simulated Annealing is another metaheuristic based method of optimization inspired by a natural phenomenon like Genetic Algorithm. While genetic algorithms are inspired by biological evolution, simulated annealing is motivated by the metallurgical process of annealing. When a piece of metal is heated, its potential energy increases and crystal structures become randomized. As it is cooled the mobility of atoms decreases and crystal structures start forming. The nature of crystals formed depends on the rate of cooling. If cooled slowly there is enough time for crystal growth and ordered crystal structues are formed, whereas if cooled very fast, a randomized structure is obtained as there is no time for the crystals to grow.   </p>
  <p>
    
    <img src="images/fast vs slow annealing.png" alt="demo image">  
    <br>
    Left image shows randomzied crystals due to fast cooling whereas the right image shows ordered crystals when cooled slowly
  </p>
  <h3>Optimization Ideas </h3>
  <h4>Energy Distribution</h4>
  <p>Any system in thermal equilibrium at temperature T has its energy distributed according to the Boltzman distribution. The ratio of probabilities of state i and j depends only on the energy difference of the two states. 
   $$ \dfrac {p_i} {p_j} = e^{- \dfrac {\Delta E} {KT} } $$
  <p>
  <img src="images/Boltzmann.png" alt="demo image">  
    <br>
    We observe that as the temperature falls, the probability ratio tends towards 0. This means that probability of state i becomes extremely low and the entire energy can be deterministically said to be at state j. This transition happens more rapidly when the energy difference between the states is large. This is intuitive too as the randomness is present only in nearby states unless temperature is very high.
    <h4>Translating ideas into algorithm</h4>
    <p>
    Imagine now we wish to minimize a multimodal function in a certain search space. The function can be either discrete or continuous. We choose a certain initial point x0 and a high temperature parameter value T0. Now, we generate another point in the neighbourhood of the initial point x1. We can imagine these points as states and the value of the function at these points to be their corresponding energies say f(x0) and f(x1). Now we have to decide if we should move to point x1 or reject it. If the value of the function decreases we accept it similar to other search methods. On the other hand, if the value of the function increases we don't reject it straight away as we would do with conventional algorithms. Instead we reject it with a probability. This enables the algorithm to not get stuck at a local minima as sometimes steps are taken which increases function value. To control the rejection probability we model it by the Boltzmann distribution parametrized by a temperature parameter. Initially when temperature is high, exploration of state space is high but as the temperature decreases it accepts lesser and lesser values when function value increases and ultimately homes in on a certain minima. If the temperature is reduced slowly, enough exploration is done, so most likely it will find the global minima approximately.</p>
  <h4>Demo Problem</h4>
  <p>Let us try to find the global minima of a multimodal function. Although the algorithm described here is equally applicable for higher dimensions as well as discrete functions we choose a 1D continuous function for keeping things simple. Let us define the optimization problem and visualize the objective function to see why it cannot be solved by traditional algorithms<br>
 $$ Minimize \: f(x) = sin(x) + sin ( \dfrac{10} {3} x) \\ subject \; to \; 0 \le x \le 6 $$ </p>
 <p> 
    <img src="images/fx.png" alt="demo image"> 
    <br>
    We see that the function has three local minimas and 3 local maximas
    </p>
    <h4>Generating the neighbours and choosing parameters</h4>
    <p>
    If we had a discrete function then it can be represented by a graph which clearly shows which states are reachable from a given state. Since we have a continuous function we need a judicious heuristic for generating the next state. One way to do this would be to pick an initial point at random from the given range and then model a Gaussian distribution around this initial point set as the mean. We know that nearby point probabilities are greater than far away ones, so we can set three times the standard deviation as half the difference between upper and lower bounds of the function. When we sample from this distribution we will get a point within the range 99% of the time. In case we get a point outside the bound we simply resample. There are several techniques of sampling from a given distribution like Box Mueller, rejection sampling and Hastings Metropolis algorithm for large dimensional problems. In this example we will simply use the built in numpy function.</p>
  <dt-code block language="python">
    def ptsampler(x0,x_bounds,n): 
        sd=(x_bounds[1]-x_bounds[0])/6.
        temp=np.random.normal(loc=x0, scale=sd, size=n)
        while(np.min(temp)<x_bounds[0] or np.max(temp)>x_bounds[1] ):
            temp= np.random.normal(loc=x0, scale=sd, size=n)
        return temp
  </dt-code>  
  <p> This function takes an initial point x0, the lower and upper bounds in x_bounds and number of samples to generate as n. It sets the mean as x0 and three times standard deviation as half of the domain length and returns n samples satisfying the constraints. The values of the function are less than 10 and we may choose the starting temperature as 1. The cooling schedule too is a hyper parameter we need to choose. There are variations like Thermodynamic Simulated Annealing where temperature is modelled as a complex function of the states. In this case we simply reduce the temperature 1.8 times after every 4 iterations. The stopping criteria is a maximum number of iterations or when the temperature falls below \( 10^{-5} \).</p>
  <h4> The acceptance criteria </h4>
  <p>After generating the point x1 we calculate f(x1)-f(x0). If this is positive the point is accepted and the process is repeated. If this is negative we calculate \( p = e^{- \dfrac{ f(x1) - f(x0) } {T} } \) <br> We generate a random number r between 0 and 1. If p is greater than r then x1 is also accepted despite the functional value increasing. If r is greater than p, the point x1 is rejected and the process starts again. Note that in the implementation, we need not check if f(x1)-f(x0) is positive as in this the p>1 and this will always be accepted. This is implemented by the two functions acceptprob and acceptreject </p>.
 <dt-code block language="python">
def f(x):
    return np.sin(x)+np.sin(10/3*x)
def acceptprob(x,T):
    if(T>0):
        return math.exp(-x/T)
    else:
        return 1.
def acceptreject(x1,x2,T):
        t1=np.random.uniform(0,1)
        if(acceptprob(f(x2)-f(x1),T)>t1):
        	return True
        else:
        	return False

  </dt-code>  
  <h4>Putting it together</h4>
  <dt-code block language="python">
x_bounds=np.array((0,6))
x0=2.5
T0=1.
kmax=100
k=0
ak=0
lt=[]
xj=x0
lt.append(xj)
while k < kmax and T0 > 1e-5:
    xk=ptsampler(xj,x_bounds,1)[0]
    if(acceptreject(xj,xk,T0)):
        xj=xk
    else:
        xk=xj
    if(xj!=lt[-1]):
        lt.append(xj)
        k+=1
    if((ak+1)%4==0):
        T0/=1.8
    ak+=1
  </dt-code>
 <p> Here we keep reducing the temperature even if steps are getting rejected to keep hardening the solution. x0 is chosen to be 2.5 which is a hard point for the algorithm as there are two local minimas on either side of it. We stop the iteration when either there are 100 accepted points or the temperature falls below \( 10^-5 \). Running this once we see it reaches x=5.14 after 17 steps. From the graph of the function we can see that this is approximately where the global minima lies.</p>
 
   
    <p>
    We see the temperature decreases and finally the global minima is found. For a robustness check we initialize the starting point with 3 different values and compare the number of steps required. All other hyper parameters are kept fixed.</p>
    
    <p>
    
    <style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax">Starting Point</th>
    <th class="tg-0lax">Global Minima</th>
    <th class="tg-0lax">Number of Steps</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">1</td>
    <td class="tg-0lax">5.13</td>
    <td class="tg-0lax">18</td>
  </tr>
  <tr>
    <td class="tg-0lax">2.5</td>
    <td class="tg-0lax">5.14</td>
    <td class="tg-0lax">17</td>
  </tr>
  <tr>
    <td class="tg-0lax">6</td>
    <td class="tg-0lax">5.15</td>
    <td class="tg-0lax">12</td>
  </tr>
</tbody>
</table>

    </p>
    <p>We observe that the closer initial value is to the global minima, lesser is the number of steps required</p>
    <img src="images/x1.gif" alt="demo image" width="30%" height="30%">  <img src="images/x25.gif" alt="demo image" width="30%" height="30%"> <img src="images/x6.gif" alt="demo image" width="30%" height="30%"> 
    <br>

    <p>-By R Sainiranjan, Sayantan Halder and Kaushal Bhogale</p>
</dt-article>
