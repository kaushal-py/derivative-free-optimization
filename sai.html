<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<dt-article>
  <h1>Derivative-free Optimization</h1>
  <h2>Tutorial for Genetic algorithms and Simulated Annealing</h2>
  <dt-byline></dt-byline>

  <h2>Genetic Algorithms</h2>
  
  <h3> Optimization - a very , very brief primer! </h3>
  
  So really, what is optimization? Since this post talks about <i> Derivative - Free Optimization </i> so to speak, it is assumed that the reader is familiar with the basics of optimization. If you are not, no need to fret thi section provides a very, very brief intro, frankly, 
  there is no need as such to have a thorough foundation on the underlying mechanism of optimization to understand the following discussion. But like many frameworks, the entire reason why Derivative free methods exist, such as the ones discussed here, Genetic Algorithms and Simulated Annealing
  is based on some of the issues encountered in the old-school methods. So it would be a great idea to have some superficial understanding of what's going on, just so that we can appreciate the innovative ideas that have solved them </p>
  
  <p> Now, to business. Optimization at a very , very basic level is to solve a certain problem <i> optimally </i>. For example, if you have your final exams for the semester, How would you try to maximize your marks? Or how would a logistics company
      decide to deliver parcels in a particular city while ensuring that it takes the least amount of time ad fuel , cause well, time is money and gasoline ain't cheap! There are many, many such problems at the heart of the modern world
      Many modern techniques such as the ever-so popular Machine Learning that has become so omnipresent today from chatbots and virtual assistants to giving Recommendations on Netflix on what to watch next are also at their very core - optimization paradigms
      Since optimization is so useful, it stands to reason that bottlenecks affecting optimization procedures should be quite important as well and indeed they are, as a matter of fact billions of dollars are spent in R&D just to get rid of these bottlenecks.
      One such bottleneck that has stymied the devolopment of better algorithms has been the over-reliance on <i> "derivatives" </i>. An enormous majority of today's optimization algorithms including those balmy Neural - Networks rely a great deal on <i> Derivative </i> based
      methods. To understand what a derivative based method does is that given some function that you want to optimize (<i> henceforth : the objective function </i>) say, $$ f(x) $$
      it uses its derivative to get some information to reach the optimal solution. High school math is enough to let us know that $$ f'(x) = 0 $$ at an extremum and a basic knowledge of multivariable calculus extends this to $$ \nabla f(\bf{x}) = \bf{0} $$
      If you want more information to <i> converge </i> faster to a solution, then you can use higher derivatives. Modern methods like Automatic Differentiation makes computing derivatives easy and since the algorithms using these
      are straightforward to implement and have robust guarantees of <i> "converging" </i> , they are used quite a great deal. Even so, they do suffer from certain issues that necessitates the ideas discussed in our post. We will be looking into those in the next section, and that's all for
      the basics of Optimization. For a more detailed introduction, there are some great videos and courses out there on You tube , not to mention the bible - Numerical Optimization by Nocedal and Wright which is a great read but a bit too mathy which might be off-putting for first time readers.
    
</p>

<h3> Why do we need Derivative free methods? </h3>
<p> <img src="images/ga_motivation.jpg" alt="GA">
  
  <br>
    
  </p>
  
  <p> The above figure shows a realistic situation that happens when we  </p>

  <p>TODO: Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magni ad esse odio repudiandae quidem porro, laborum vel aspernatur praesentium deleniti quaerat commodi blanditiis, natus cum impedit aperiam voluptatem architecto. Maiores.</p>
  <h3>Subheading</h3>
  <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Odio dolor delectus tenetur ex quibusdam esse voluptatibus corrupti incidunt, suscipit inventore, ducimus fugit sunt in ipsam ratione. Deserunt dolor eius minima.</p>
  <h4>Sub-subheading</h4>
  <p>Math equations inline can be written as: \( a=b+c+\dfrac{e}{f} \). Check index.html (line 28) for syntax.</p>
  <p>Write Block level math equations using double dollars. 
    $$ a \cdot x^2 + b \cdot x + c = 0 $$
  </p>

  <p>Template for inserting code</p>
  <dt-code block language="python">
    import numpy as np
    import matplotlib.pyplot as plt

    x = np.arange(10)
    y = np.arange(10)+np.random.randn(10)
    plt.plot(x, y)
    plt.show()
  </dt-code>
  <p>
    Add images / gifs as following:
    <img src="images/demofig.png" alt="demo image">
  </p>
  
  
  <p>Lorem, ipsum dolor sit amet consectetur adipisicing elit. Et ad, ducimus dolorum nisi quidem inventore. Pariatur illo quis quia ratione provident labore id? Ea reiciendis saepe dolore commodi expedita placeat.</p>
  <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>

  <h2>Simulated Annealing</h2>
  <p>TODO: Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magni ad esse odio repudiandae quidem porro, laborum vel aspernatur praesentium deleniti quaerat commodi blanditiis, natus cum impedit aperiam voluptatem architecto. Maiores.</p>
  <h3>Subheading</h3>
  <h4>Sub-subheading</h4>
  <p>Lorem, ipsum dolor sit amet consectetur adipisicing elit. Et ad, ducimus dolorum nisi quidem inventore. Pariatur illo quis quia ratione provident labore id? Ea reiciendis saepe dolore commodi expedita placeat.</p>
  <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>

</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
</script>
