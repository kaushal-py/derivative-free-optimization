<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<dt-article>
  <h1>Derivative-free Optimization</h1>
  <h2>Tutorial for Genetic algorithms and Simulated Annealing</h2>
  <dt-byline></dt-byline>

  <h2>Genetic Algorithms</h2>
  
  <h3> Optimization - a very , very brief primer! </h3>
  
 <p>  So really, what is optimization? Since this post talks about <i> Derivative - Free Optimization </i> so to speak, it is assumed that the reader is familiar with the basics of optimization. If you are not, no need to fret this section provides a very, very brief intro, frankly, 
  there is no need as such to have a thorough foundation on the underlying mechanism of optimization to understand the following discussion. But like many frameworks, the entire reason why Derivative free methods exist, such as the ones discussed here, Genetic Algorithms and Simulated Annealing
  is based on some of the issues encountered in the old-school methods. So it would be a great idea to have some superficial understanding of what's going on, just so that we can appreciate the innovative ideas that have solved them </p>
  
  </p>
  
  <p> Now, to business. Optimization at a very , very basic level is to solve a certain problem <i> optimally </i>. For example, if you have your final exams for the semester, How would you try to maximize your marks? Or how would a logistics company
      decide to deliver parcels in a particular city while ensuring that it takes the least amount of time ad fuel , cause well, time is money and fuel ain't cheap! There are many, many such problems at the heart of the modern world
      Many modern techniques such as the ever-so popular Machine Learning that has become so omnipresent today from chatbots and virtual assistants to giving Recommendations on Netflix on what to watch next are also at their very core - optimization paradigms
      Since optimization is so useful, it stands to reason that bottlenecks affecting optimization procedures should be quite important as well and indeed they are, as a matter of fact billions of dollars are spent in R&D just to get rid of these bottlenecks.
      One such bottleneck that has stymied the devolopment of better algorithms has been the over-reliance on <i> "derivatives" </i>. An enormous majority of today's optimization algorithms including those all-powerful Neural - Networks rely a great deal on <i> Derivative </i> based
      methods. To understand what a derivative based method does consider the following ; given some function that you want to optimize (<i> henceforth : the objective function </i>) say, $$ f(x) $$
      the method would use its derivative to get some information to reach the optimal solution. High school math is enough to let us know that $$ f'(x) = 0 $$ at an extremum and a basic knowledge of multivariable calculus extends this to $$ \nabla f(\bf{x}) = \bf{0} $$
      If you want more information to <i> converge </i> faster to a solution, then you can use higher derivatives or their approximations. Modern methods like Automatic Differentiation makes computing derivatives easy and since the algorithms using these
      are straightforward to implement and have robust guarantees of <i> "converging" </i> , they are used quite a great deal. Even so, they do suffer from certain issues that necessitates the ideas discussed in our post. We will be looking into those in the next section, and that's all for
      the basics of Optimization. For a more detailed introduction on optimization and derivative based methods, there are some great videos and courses out there on You tube , not to mention the bible - Numerical Optimization by Nocedal and Wright which is a great read.
    
</p>

<h3> Why do we need Derivative free methods? </h3>
<p> <img src="images/ga_motivation.jpg" alt="GA">
  
  <br>
    
  </p>
  
  <p> The above figure shows a realistic situation that happens when we use Derivative based methods. If you could recall from above, the idea behind derivative based methods is to use characteristics
      of the derivatives of the objective function to get closer to an optimal value. This works quite well for your usual sort of functions that are well behaved such as convex functions ("Bowl shaped functions" : Your task 
      is to reach the bottom of the bowl, the derivative provides the direction of steepest descent and then whoosh! Off you go! Just to see why this is so fast, take a bowl and use your fingers to replicate the process, you woulkd reach the bottom in no time!) b
      but goes wonky for other functions that are not so well behaved. This is because "derivatives" are unable to distinguish between <i> locally </i> and <i> globally </i> optimal solutions. 
           
  </p>

  <p> To understand what this means, look at the figure above, Suppose you are a mountaineer out to conquer the highest mountain on a range, you have got a bunch of peaks there on your landscape. Look at the point labelled <i> Start Here Randomly </i>, what derivative based methods would do is to start climbing upward,
      they would ensure that you get to the top of the nearest hill, but then you're stuck! You can't take a single step ahead because the derivatve which is sort of a slope monitor tells you that the terrain doesn't go upward from here !
      So according to your monitor you have reached the peak, but you being a well-prepared person have already done your survey and are aware beforehand that there is a much higher peak ahead! So now you are stuck and angry, but fret not
     there were many others in the same fix as you were while going through different applications in engineering and science, applications that are quite necessary to the functioning of various aspects of society that we take for granted today.
     So the answer to this problem is to include an entirely new monitor to understand the terrain - i.e, to consciously chose to avoid using the <i> derivative </i> which is why these methods are called derivative-free methods.
     There are other reasons to adopt derivative-free based methods but they are mostly <i> computational </i> in nature i.e relate to the problems encountered in getting the derivative in the first place. The above discussed "landscape problem "
     shows on the other hand a much more intuitive need for going "beyond" the derivative in the first place. Many of the methods we will discuss below are often grouped together in the banner of <i> meta-heuristics </i> (Meta : Latin for beyond,
     heuristics - methodology of solving a problem, in this case referring to the derivative based methods.) 
    
</p>

<p> Now, that we have a certain need for Derivative free methods. Let's get cracking ! </p>


<h3> Genetic Algorithms </h3>

<p> Before we begin, We would look to state that the <i> phenomenon </i> of optimization is far more general than we thought. While the way in which humans perform decision making can be modelled as an optimization scenario, the broader behaviour of nature may be modelled 
    through an optimization perspective. To understand what we are trying to say, look at the figure below
  
   <img src="images/PSO.jpg" alt="PSO"> <br>
  
  The image shows a flock of birds trying to find some fruit to eat. From observation, people observed that any movement of the flock followed three simple rules : 
  
  <ul> <li> Move in the same direction as your neighbours </li>
       <li> Stay close to your neighbours </li>
       <li> Avoid collisions with your neighbours   
  </ul>

  Surprisingly, these simple rules ensure that the flock finds food in the shortest period of time! What we have therfeore is an optimization paradigm where the problem is to find sufficient food for the entire flock in the shortest possible of time! Many such examples
  are abound in nature and since the behaviour of nature is often based on certain simple rules (Wouldn't be so widespread otherwise! If birds had to sit down and run robust calculations to calculate how to get food the flock would have died out long ago), they tend to be 
 <i> derivative -free </i> and quite general. For example , the following amazing article <a href = "https://www.quantamagazine.org/math-of-the-penguins-20200" >Math of the Penguins </a> shows how a flock of Emperor penguins huddle in the most efficient structure possible in order to keep warm
 in the frigid winter, all based on the simple rule that every penguin wants to stay warm and would thus migrate to a warmer location to get the warmth it needs. The innate simplicity, generality and the <i> understanding </i> that such methods can converge to the optimal solution inspired scientists 
 and engineers to design many nature inspired algorithms for optimization. (Note : we mentioned that nature inspired paradigms "could" converge to the solution, till date there is no rigid theory on how and why these methods can converge thus making it more of an empirical framework which is why
 it is often not preferred despite its performance. Moreover, the selection of parameters is always a challenge. For our discussion, we would sweep such issues under the rug for the most part while keeping in mind that they exist, since these methods are quite effective practically which makes it worth
 to have a proper discussion!)

</p>

<p> The paradigm that we are going to discuss in this section - namely, Genetic Algorithms are a specific kind of nature inspired optimization algorithms which are inspired from genetics and Darwin's theory of evolution </p>

<h4> Another brief summary - This time a primer on biology! </h4>

<p> The theory of genetics and Darwin's theory on evolution are so widespread that an introduction sounds redudant. Even so, we provide a brief refresher in this section with the definition of some terms that will be encountered in our discussion but are not so well known.
</p>

<p>
  
The genetic basis of inheritance suggests that the way individuals of a species pass on their traits to their offspring is through information encoded in their genes, which are basically sections of chromosones which are themselves a bunch of DNA molecules, found in the cells
of the individual organisms. How genes interact in order to come up with the various traits such as height or colour eyes is beyond the scope of any introductory material but we may assume for the sake of simplcity that genes have certain possible values termed as <i> alleles </i>.
Different alleles in a chromosome together determine how a certain trait is expressed. We define any observable characteristic such as height or the colour of eyes to be a <i> phenotype </i>. All the alleles displayed in the various chromosomes that determine a phenotype are the 
<i> Genotype </i>.
  
</p>

<p>
  
  
Now, we come to the discussion on evolution. If we assume that the genotype of an organism remains constant throughout its existence then we have no diveristy - which is not ideal given that the environment is always changing and forcing the organism and the species to respond in different ways.
Thus, the changing environment throughout the course of our history has forced organisms to respond in different sorts of ways in order to avoid becoming extinct , i.e by showcasing different phenotypes which implies that we need different genotypes. Thus, we come to the concept of darwinian evolution, i,e "Survial of the fittest", 
Therefore "evolution" which for our case we assume has the meaning that the organism <i> shows the optimal fitness in a given environment </i> has to proceed by a modification of the genotype guided by some <i> fitness measure</i> so that the organism can survive better. 
Since, the genotype has to be modified in some manner, how can organisms go about doing this? The first and the easiest way, is to change the genotype on their own, since there is no explicit way for organisms to explicitly tell their genotype to change in a certain way, any such change has to be <i> random </i>  in nature(Btw , 
if you are interested the recent Nobel Prize in medicine was given for the CRISPR method, is an innovative method that allows for explicit editing of the genotype). Any such random change or <i> mutation </i> (You would have encountered this term in the X-Men movies for example, with exactly the same meaning
!) allows the organism to showcase different phenotypes, allowing it to adapt better to the changing environment during its lifetime. But clearly , mutation changes do not seem to allow for increase in fitness of the species as whole! Thw way nature solves this problem is by ensuring that during sexual reproduction when new
offpsring are generated, there is a significant possibility for traits from either of the two parents to be passed on to the offspring, again as there is no regulatory procedure which trait gets passed on to which offspring is entirely stochastic however over a long time span the proportion of the benificial phenotype will be magnified
since the organisms without the said benificial trait would have died out! This explains how evolution and extinction of species occurs in nature. This transfer of trait from the genes of the two parents is termed as "crossing-over" (a detailed description of the nomenclature would require a digression on meiosis which is the type of cell
division occuring in the reproductive or "germinal" cells such as the sperm or the egg.) Intutively, one can think of the crossing over process in the following manner : keep two chromosomes next to each other take some alleles from one and interchange it with the other to get a certain resultant chromosome. This is also made clear in the figure below:

  
<img src = "images/crossing_over.png" , alt = "crossing over"> <br>
  
The above image shows how crossing over occurs in genes during cell division. That's all the information we would need from biology. Now with the intution in place let's get to how we would be actually using "Genetic Algorithms" 
  
</p>

<h4> The formulation of Genetic Algorithms </h4>
  
  
  
  
  
  <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>

  <h2>Simulated Annealing</h2>
  <p>TODO: Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magni ad esse odio repudiandae quidem porro, laborum vel aspernatur praesentium deleniti quaerat commodi blanditiis, natus cum impedit aperiam voluptatem architecto. Maiores.</p>
  <h3>Subheading</h3>
  <h4>Sub-subheading</h4>
  <p>Lorem, ipsum dolor sit amet consectetur adipisicing elit. Et ad, ducimus dolorum nisi quidem inventore. Pariatur illo quis quia ratione provident labore id? Ea reiciendis saepe dolore commodi expedita placeat.</p>
  <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>

</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
</script>
